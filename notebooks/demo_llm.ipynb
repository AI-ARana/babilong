{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets torch openai tiktoken nltk seaborn matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/booydar/babilong source\n",
    "!unzip source/data/tasks_1-20_v1-2.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import time\n",
    "import os\n",
    "import datasets\n",
    "from source.babilong_utils import TaskDataset, SentenceSampler, NoiseInjectionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'qa1'\n",
    "model = 'gpt-4-1106-preview'\n",
    "message_length = 4000\n",
    "number_of_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, model):\n",
    "        self.impl_ = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        if isinstance(inp, list):\n",
    "            result = self.impl_.encode_batch(inp)\n",
    "        else:\n",
    "            result = self.impl_.encode(inp)\n",
    "        return {\n",
    "            'input_ids': result\n",
    "        }\n",
    "\n",
    "    def encode(self, inp, add_special_tokens):\n",
    "        return self.impl_.encode(inp)\n",
    "\n",
    "    def decode(self, inp):\n",
    "        return self.impl_.decode(inp)\n",
    "\n",
    "    def decode_batch(self, inp):\n",
    "        return self.impl_.decode_batch(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(task, exist_ok=True)\n",
    "\n",
    "with open('token') as inp:\n",
    "    key = inp.read().strip()\n",
    "\n",
    "client = openai.OpenAI(api_key=key)\n",
    "\n",
    "outfile =  f'{task}/msg_{message_length}.csv'\n",
    "df = pd.DataFrame({\n",
    "    'answer': [],\n",
    "    'gpt4answer': [],\n",
    "    'result': [],\n",
    "})\n",
    "\n",
    "test_path = 'data/tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "\n",
    "noise_dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "#noise_dataset = datasets.load_from_disk(\"pg19-data-test\")\n",
    "\n",
    "tokenizer = Tokenizer(model)\n",
    "\n",
    "# task \n",
    "task_dataset_test = TaskDataset(test_path)\n",
    "\n",
    "\n",
    "noise_sampler_test = SentenceSampler(noise_dataset['test'], tokenizer=tokenizer)\n",
    "\n",
    "dataset_test = NoiseInjectionDataset(task_dataset=task_dataset_test,\n",
    "                                        noise_sampler=noise_sampler_test,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        sample_size=message_length)\n",
    "\n",
    "\n",
    "for i, sample in zip(range(number_of_samples), dataset_test):\n",
    "    facts = sample['facts']\n",
    "    question = sample['question']\n",
    "    true_answer = tokenizer.decode(sample['target_tokens'])\n",
    "    background_text = tokenizer.decode_batch(sample['background_text'])\n",
    "    query = tokenizer.decode(sample['input_tokens'])\n",
    "    \n",
    "    messages = [ \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a intelligent assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \n",
    "                \"I give you context with the facts about positions of different persons hidden in some random text and a question. \"\n",
    "                \"You need to answer the question based only on the information from the facts. \"\n",
    "                \"If a person was in different locations use the latest location to answer the question.\\n\\n\"\n",
    "                \"<example>\\n\"\n",
    "                f\"Charlie went to the hallway. Judith come back to the kitchen. Charlie travelled to balcony. Where is Charlie?\\n\"\n",
    "                \"Assistant: balcony\\n\"\n",
    "                \"</example>\\n\\n\"\n",
    "                \"<example>\\n\"\n",
    "                f\"Alan moved to the garage. Charlie went to the beach. Alan went to the shop. Rouse travelled to balcony. Where is Alan?\\n\"\n",
    "                \"Assistant: shop\\n\"\n",
    "                \"</example>\\n\\n\"\n",
    "                \"<context>\\n\"\n",
    "                f\"{query}\"\n",
    "                \"</context>\\n\\n\"\n",
    "                f\"QUESTION: {question}\\n\" \n",
    "                \"Your answer should be a single word - the most recent location of ’person’ in the question. \"\n",
    "                \"Do not write anything afer that.\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "    gpt_answer = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    if gpt_answer.endswith('.'):\n",
    "        gpt_answer = gpt_answer[:-1]\n",
    "\n",
    "    print(i, true_answer, gpt_answer)\n",
    "\n",
    "    df.loc[len(df)] = [true_answer, gpt_answer, true_answer == gpt_answer]\n",
    "    df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((1, 7))\n",
    "\n",
    "for i, msg in enumerate([0, 4, 8, 16, 32, 64, 128]):\n",
    "    msg = msg * 1000\n",
    "    fname = f'{task}/msg_{msg}.csv'\n",
    "    if not os.path.isfile(fname):\n",
    "        print('not such file', fname)\n",
    "        continue     \n",
    "    df = pd.read_csv(fname, index_col=[0])\n",
    "    last_word = df['gpt4answer'].apply(lambda x: x.split(' ')[-1]).apply(lambda x: x.split('\\n')[-1])\n",
    "    score = (last_word == df['answer']).sum()\n",
    "    results[0, i] = score / len(df)\n",
    "\n",
    "font = {\n",
    "    'size'   : 30\n",
    "}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 2)\n",
    "\n",
    "\n",
    "ax = sns.heatmap(results, annot=True, linewidth=0.5)\n",
    "ax.set_xlabel('Context length')\n",
    "ax.set_xticks(np.array(range(results.shape[1])) + 0.5, ['0k', '4k', '8k', '16k', '32k', '64k', '128k'])\n",
    "ax.tick_params(axis='y', rotation=90)\n",
    "ax.set_yticks([0], [''])\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('fig.pdf', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babilong",
   "language": "python",
   "name": "babilong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
