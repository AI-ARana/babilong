# BABILong - a long-context benchmark for LLM evaluation using the needle-in-a-haystack approach.

**BABILong** is a novel generative benchmark for evaluating the performance of NLP models in
processing arbitrarily long documents with distributed facts.

Solving tasks with a long context size requires the model to distinguish important information from large amounts of irrelevant details. To simulate this behavior we ”hide” the sentences of the original task between the sentences of irrelevant text. We use the [bAbI](https://huggingface.co/datasets/facebook/babi_qa) dataset as facts and [PG19](https://huggingface.co/datasets/pg19) as background text.

<img src="images/babilong_scheme.png" alt="drawing" width="400"/>

BABILong consists of 20 tasks designed for evaluation of basic aspects of reasoning. The
bAbI tasks are generated by simulating a set of characters and objects engaged in various movements and interactions with each other in multiple locations. Each interaction is represented by a fact, e.g. **”Mary travelled to the office”**, and the task is to answer a question using the facts from the current simulation, for instance, **”Where is Mary?”**. The
bAbI tasks vary based on the number of facts, question complexity and the aspects of reasoning.

## [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/booydar/babilong/blob/main/notebooks/babilong_usage_example.ipynb) Get acquainted with BABILong

## [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/booydar/babilong/blob/main/notebooks/demo_llm.ipynb) Evaluate your long-context model


<img src="images/results.png" alt="drawing" width="800"/>
